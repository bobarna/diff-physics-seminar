\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ControlPDEs}
\citation{ControlPDEs}
\citation{ControlPDEs}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem}{1}{section.2}\protected@file@percent }
\newlabel{eq:problem-pde}{{1}{1}{Problem}{equation.2.1}{}}
\newlabel{eq:problem-pde}{{2}{1}{Problem}{equation.2.2}{}}
\newlabel{eq:problem-loss-match-state}{{3}{1}{Problem}{equation.2.3}{}}
\newlabel{eq:problem-loss-force}{{4}{1}{Problem}{equation.2.4}{}}
\newlabel{eq:loss-function}{{5}{1}{Problem}{equation.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Preliminaries}{1}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Differentiable Solvers}{1}{subsection.3.1}\protected@file@percent }
\citation{ControlPDEs}
\citation{ControlPDEs}
\citation{ControlPDEs}
\citation{ControlPDEs}
\citation{unet}
\citation{residual-blocks}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Iterative trajectory optimization}{2}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Execution order}{2}{subsection.4.1}\protected@file@percent }
\citation{tensorflow}
\citation{ControlPDEs}
\citation{ControlPDEs}
\citation{ControlPDEs}
\citation{ControlPDEs}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Learning to throw. An example, showcasing an important difference between supervised and differentiable physics approaches. Both the supervised and the differentiable physics network approximate the function $f^{-1}(\vb {x_{final}}): \mathbb  {R} \DOTSB \mapstochar \rightarrow \mathbb  {R}^4$, which is the inverse of the function $f(\vb {x}, \vb {y}, \vb {v}, \vb {\alpha })$, mapping the final position $\vb {x_{final}}$ of an object being thrown from position $(\vb {x}, \vb {y})$, with velocity $\vb {v}$ and angle $\vb {\alpha }$. The same network architecture is used, with the weights initialized to the same initial values. Both networks have seen the same number of training examples, and are using an $L_2$ norm between the point of impact resulting from the predicted initial values and the intended position. It is evident that the DP network is able to get orders of magnitude closer than the supervised network, which has no knowledge of the underlying physical system, and it's best guess is to interpolate between the closest data it has seen during training, which results in a coarse approximation. Also, as the result space to this problem is not unimodal (e.g. it has multiple possible right answers), the supervised model is further thrown off, and will give values in-between. This means that even if we increase the training data, our supervised model can never learn this problem properly. \relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:learning-to-throw}{{1}{3}{Learning to throw. An example, showcasing an important difference between supervised and differentiable physics approaches. Both the supervised and the differentiable physics network approximate the function $f^{-1}(\vb {x_{final}}): \mathbb {R} \mapsto \mathbb {R}^4$, which is the inverse of the function $f(\vb {x}, \vb {y}, \vb {v}, \vb {\alpha })$, mapping the final position $\vb {x_{final}}$ of an object being thrown from position $(\vb {x}, \vb {y})$, with velocity $\vb {v}$ and angle $\vb {\alpha }$. The same network architecture is used, with the weights initialized to the same initial values. Both networks have seen the same number of training examples, and are using an $L_2$ norm between the point of impact resulting from the predicted initial values and the intended position. It is evident that the DP network is able to get orders of magnitude closer than the supervised network, which has no knowledge of the underlying physical system, and it's best guess is to interpolate between the closest data it has seen during training, which results in a coarse approximation. Also, as the result space to this problem is not unimodal (e.g. it has multiple possible right answers), the supervised model is further thrown off, and will give values in-between. This means that even if we increase the training data, our supervised model can never learn this problem properly. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Architecture and training}{3}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{3}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Burger's Equation}{3}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Incompressible fluid flow}{3}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Incompressible fluid with indirect control}{3}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Simulation Control}{3}{subsubsection.5.3.1}\protected@file@percent }
\citation{ControlPDEs}
\citation{ControlPDEs}
\bibstyle{ACM-Reference-Format}
\bibdata{bibliography}
\bibcite{tensorflow}{{1}{2016}{{Abadi et~al\mbox  {.}}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia, Jozefowicz, Kaiser, Kudlur, Levenberg, Mane, Monga, Moore, Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan, Viegas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng}}}
\bibcite{residual-blocks}{{2}{2015}{{He et~al\mbox  {.}}}{{He, Zhang, Ren, and Sun}}}
\bibcite{ControlPDEs}{{3}{2020}{{Holl et~al\mbox  {.}}}{{Holl, Koltun, and Thuerey}}}
\bibcite{unet}{{4}{2015}{{Ronneberger et~al\mbox  {.}}}{{Ronneberger, Fischer, and Brox}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{4.185pt}
\newlabel{tocindent2}{10.34999pt}
\newlabel{tocindent3}{18.198pt}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  An example of training the network for shape transition, advancing from the initial state $\vb {o_0}$ to the desired state $\vb {o_*}$ through some reconstructed states $\vb {o}(u(t_i))$ from left to right. This example was created using a $64 \times 64$ grid. \relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:shape-transition}{{2}{4}{An example of training the network for shape transition, advancing from the initial state $\vb {o_0}$ to the desired state $\vb {o_*}$ through some reconstructed states $\vb {o}(u(t_i))$ from left to right. This example was created using a $64 \times 64$ grid. \relax }{figure.caption.3}{}}
\newlabel{fig:indirect}{{\caption@xref {fig:indirect}{ on input line 451}}{4}{Simulation Control}{figure.caption.4}{}}
\newlabel{fig:indirect-a}{{3a}{4}{start\relax }{figure.caption.4}{}}
\newlabel{sub@fig:indirect-a}{{a}{4}{start\relax }{figure.caption.4}{}}
\newlabel{fig:indirect-b}{{3b}{4}{\relax }{figure.caption.4}{}}
\newlabel{sub@fig:indirect-b}{{b}{4}{\relax }{figure.caption.4}{}}
\newlabel{fig:indirect-c}{{3c}{4}{end\relax }{figure.caption.4}{}}
\newlabel{sub@fig:indirect-c}{{c}{4}{end\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Incompressible fluid with indirect control. The agent is able to exert force in the blue region, and is able to observ the density $\rho $, denoted with yellow. The objective is to get all of the material out on a predefined "bucket"\relax }}{4}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Source}{4}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Comparison with Existing Methods}{4}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{4}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Acknowledgments}{4}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{4}{section*.8}\protected@file@percent }
\newlabel{TotPages}{{4}{4}{}{page.4}{}}
\gdef \@abspage@last{4}
